[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Welcome to my CV page. Below, you can find a preview of my CV and a link to download the full document.\n\n\nDownload My CV\nDownload my CV as a PDF\n\n\n\nPreview\n\n\nIf the preview does not load, you can download the CV here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\n\nBiography\nI am currently a PhD student at the University of Edinburgh studying Youth in Transition: Longitudinal Comparisons of Youth Transitions in the UK using Cohort and Synthetic Cohort Data. I am particularly interested in social stratification, social class, and social mobility. The key substantive focus of my research is the sociology of social stratification. My research draws on a range of quantitative methodological techniques, including but not limited to: logistic regression, continuation ratio models, sensitivity analysis, and multiple imputation. My research mainly uses longitudinal birth cohort data.\nMy PhD examines the role structural inequality has upon choice and opportunity for youth entering into the world of economic activity for the first time post-mandatory education. I study this impact within two birth cohorts - the National Childhood Development Study and the British Cohort Study. I also use the UK Household Longitudinal Study to construct synthetic cohorts to cover a period of time whereby no birth cohort data is present. My PhD also compares cross-cohort differences within a socio-historical context.\nOn this website, you will find information on my research. Please feel free to contact me if you have any questions or would like to discuss potential projects."
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Here is a list of academic conference presentations I have delivered:\n\n\n\nConference: Stirling Social Stratification Conference\nDate: 2025\nLocation: Stirling, Scotland\n\nView Slides\n\n\n\n\n\nConference: Stirling All Hands Seminar\nDate: 2025\nLocation: Stirling, Scotland\n\nView Slides\n\n\n\n\n\nConference: Large Social Survey Symposium\nDate: 2025\nLocation: Edinburgh, Scotland\n\nView Slides\n\n\n\n\n\nConference: Intersectional analysis and quantitative methods\nDate: 2025\nLocation: Loughborough, England\n\nView Slides\nView Jupyter Notebook File\n\n\n\n\n\nConference: New Directions\nDate: 2025\nLocation: Edinburgh, Scotland\n\nView Slides\nView Github\nView Jupyter Notebook File of Simulation\nView Jupyter Notebook File of Handling Missing Data in different Software\n\n\n\n\n\nConference: Large Social Survey Symposium\nDate: 2024\nLocation: Edinburgh, Scotland\n\nView Slides\nView .do File\nView Jupyter Notebook File\n\n\n\n\n\nConference: Social Stratification Seminar\nDate: 2024\nLocation: Stirling, Scotland\n\nView Slides\nView Github\n\n\n\n\n\nConference: Large Social Survey Symposium\nDate: 2024\nLocation: Stirling, Scotland\n\nView Slides\nView Github\n\n\n\n\n\nConference: British Journal of Sociology Inaugural Conference\nDate: 2024\nLocation: London, UK\n\nView Slides\nView Github\nView App\n\n\n\n\n\nConference: New Directions\nDate: 2023\nLocation: Edinburgh, Scotland\n\nView Slides"
  },
  {
    "objectID": "presentations/index.html#youth-in-transition-longitudinal-comparisons-of-youth-transitions-in-the-uk-using-cohort-and-synthetic-cohort-data",
    "href": "presentations/index.html#youth-in-transition-longitudinal-comparisons-of-youth-transitions-in-the-uk-using-cohort-and-synthetic-cohort-data",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Social Stratification Seminar\nDate: 2024\nLocation: Stirling, Scotland\n\nView Slides\nView Github"
  },
  {
    "objectID": "presentations/index.html#how-should-we-handle-missing-data",
    "href": "presentations/index.html#how-should-we-handle-missing-data",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Large Social Survey Symposium\nDate: 2024\nLocation: Stirling, Scotland\n\nView Slides\nView Github"
  },
  {
    "objectID": "presentations/index.html#how-should-we-handle-missing-data-1",
    "href": "presentations/index.html#how-should-we-handle-missing-data-1",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Large Social Survey Symposium\nDate: 2024\nLocation: Stirling, Scotland\n\nView Slides"
  },
  {
    "objectID": "presentations/index.html#using-shiny-as-a-teaching-tool-construction-of-the-gbcs-ns-sec-and-wrights-neo-marxian-schema-as-an-open-source-class-calculator-application",
    "href": "presentations/index.html#using-shiny-as-a-teaching-tool-construction-of-the-gbcs-ns-sec-and-wrights-neo-marxian-schema-as-an-open-source-class-calculator-application",
    "title": "Presentations",
    "section": "",
    "text": "Conference: British Journal of Sociology Inaugural Conference\nDate: 2024\nLocation: London, UK\n\nView Slides\nView Github\nView App"
  },
  {
    "objectID": "presentations/index.html#youth-transitions-and-economic-activity-a-re-examination-of-ncds-data",
    "href": "presentations/index.html#youth-transitions-and-economic-activity-a-re-examination-of-ncds-data",
    "title": "Presentations",
    "section": "",
    "text": "Conference: New Directions\nDate: 2023\nLocation: Edinburgh, Scotland\n\nView Slides"
  },
  {
    "objectID": "docs/presentations/index.html",
    "href": "docs/presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Here is a list of academic conference presentations I have delivered:\n\n\nConference: Social Stratification Seminar\nDate: 2024 Location: Stirling, Scotland\nView Slides View Appendix App\n\n\n\n\nConference: Large Social Survey Symposium Date: 2024 Location: Stirling, Scotland\nView Slides\n\n\n\n\nConference: Large Social Survey Symposium Date: 2024 Location: Stirling, Scotland\nView Slides\n\n\n\n\nConference: British Journal of Sociology Inaugural Conference Date: 2024\nLocation: London, UK\nView Slides\n\n\n\n\n\nConference: New Directions\nDate: 2023\nLocation: Edinburgh, Scotland\nView Slides"
  },
  {
    "objectID": "docs/presentations/index.html#youth-in-transition-longitudinal-comparisons-of-youth-transitions-in-the-uk-using-cohort-and-synthetic-cohort-data",
    "href": "docs/presentations/index.html#youth-in-transition-longitudinal-comparisons-of-youth-transitions-in-the-uk-using-cohort-and-synthetic-cohort-data",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Social Stratification Seminar\nDate: 2024 Location: Stirling, Scotland\nView Slides View Appendix App"
  },
  {
    "objectID": "docs/presentations/index.html#how-should-we-handle-missing-data",
    "href": "docs/presentations/index.html#how-should-we-handle-missing-data",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Large Social Survey Symposium Date: 2024 Location: Stirling, Scotland\nView Slides"
  },
  {
    "objectID": "docs/presentations/index.html#how-should-we-handle-missing-data-1",
    "href": "docs/presentations/index.html#how-should-we-handle-missing-data-1",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Large Social Survey Symposium Date: 2024 Location: Stirling, Scotland\nView Slides"
  },
  {
    "objectID": "docs/presentations/index.html#using-shiny-as-a-teaching-tool-construction-of-the-gbcs-ns-sec-and-wrights-neo-marxian-schema-as-an-open-source-class-calculator-application",
    "href": "docs/presentations/index.html#using-shiny-as-a-teaching-tool-construction-of-the-gbcs-ns-sec-and-wrights-neo-marxian-schema-as-an-open-source-class-calculator-application",
    "title": "Presentations",
    "section": "",
    "text": "Conference: British Journal of Sociology Inaugural Conference Date: 2024\nLocation: London, UK\nView Slides"
  },
  {
    "objectID": "docs/presentations/index.html#youth-transitions-and-economic-activity-a-re-examination-of-ncds-data",
    "href": "docs/presentations/index.html#youth-transitions-and-economic-activity-a-re-examination-of-ncds-data",
    "title": "Presentations",
    "section": "",
    "text": "Conference: New Directions\nDate: 2023\nLocation: Edinburgh, Scotland\nView Slides"
  },
  {
    "objectID": "training/index.html",
    "href": "training/index.html",
    "title": "Training",
    "section": "",
    "text": "Here is a list of training sessions, workshops, or courses I have delivered or attended:"
  },
  {
    "objectID": "training/index.html#survey-and-questionnaire-construction-the-shiny-package-in-r",
    "href": "training/index.html#survey-and-questionnaire-construction-the-shiny-package-in-r",
    "title": "Training",
    "section": "Survey and Questionnaire Construction: the Shiny Package in R",
    "text": "Survey and Questionnaire Construction: the Shiny Package in R\nType: Workshop Delivered\nDate: November 24th 2024\nLocation: Edinburgh Futures Institute, Edinburgh, Scotland\nDescription: This training event will cover teaching users about the Shiny package within the R environment. The Shiny package allows users to create their own applications free of charge and host them online. This training will focus on a key aspect of app creation – creating one’s own survey/questionnaire that can be used in their own research agenda.\n\nView Training Materials\n\nView Example Code #1\n\nView Example Code #2\n\nView Example Code #3\n\nView Example Code #4\n\nView Example Code #5\n\nView Example Code #6\n\nView Example Code #7\n\nView Example Code #8\n\nView Example .css Code\n\nView Example .js Code\n\nView PowerPoint"
  },
  {
    "objectID": "training/index.html#jupyter-notebook-improving-workflow-for-social-science-research",
    "href": "training/index.html#jupyter-notebook-improving-workflow-for-social-science-research",
    "title": "Training",
    "section": "Jupyter Notebook: Improving Workflow for Social Science Research",
    "text": "Jupyter Notebook: Improving Workflow for Social Science Research\nType: Workshop Delivered\nDate: October 20th 2023\nLocation: Edinburgh, Scotland\nDescription: This training will focus on setting up Jupyter Notebook on individual devices and importing the user’s data analysis software of choice (course taught in Stata) into the Notebook environment. After setup is complete, training will focus on establishing the various tools that Jupyter Notebook has to offer; demonstrating how Jupyter goes beyond the clunky and sometimes dysfunctional .do files of Stata and other software providers.\n\nView Training Materials\n\nView PowerPoint"
  },
  {
    "objectID": "cv/index.html#curriculum-vitae",
    "href": "cv/index.html#curriculum-vitae",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Welcome to my CV page. Below, you can find a preview of my CV and a link to download the full document.\n\n\n\nDownload my CV as a PDF\n\n\n\n\n\n\nIf the preview does not load, you can download the CV here."
  },
  {
    "objectID": "training/index.html#introduction-to-statistics-in-stata",
    "href": "training/index.html#introduction-to-statistics-in-stata",
    "title": "Training",
    "section": "Introduction to Statistics in Stata",
    "text": "Introduction to Statistics in Stata\nType: Training Materials\nDate: September 1st 2023\nLocation: Edinburgh, Scotland\nDescription: These files will provide a holistic undergraduate level understanding of quantiative methods.\n\nView Intro to Quantitative Statistics\nView Intro to Advanced Quantitative Statistics\nView Example Dataset\n\nView Example Code #1\n\nView Example Code #2\n\nView Example Code #3"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Here is a list of my web applications, data projects, and other software solutions:\n\n\nDescription:\nThis web application was designed to aid in teaching Sociology students about the operationalisation of social class. It was built using R and the Shiny package, and it provides users with teh ability to test how different measure of social class are constructed and used in a survey environment.\n-View Project\n-Source Code\n\n\n\n\nDescription:\nThis web application was designed to aid in conference presentation, it hosts every figure/graph/table in a given paper presentation. The app itself uses R code using the Shiny package.\n-View Project\n\n\n\n\nDescription:\nThis web application was designed to showcase the verstalilty of Shiny, being able to construct surveys and save user inputs using google cloud servers to store in a remote google sheets.\n-View Project\n\n\n\n\nDescription:\nStata program to compute and analyze social stratification variables. socstrat is designed to calculate and analyze social stratification variables based on two variables: the National Statistics Socio-Economic Classification (NS-SEC) or Registrar General’s Social Class (RGSC). The command requires at least two variables to define the standard occupation classification (SOC) code and the employment status (via the socstatus() option and generates a new variable that represents the computed social stratification variable.\n-View Project\n\n\n\n\nDescription:\nDetailed Roadmap of Ethnicity Data Collection in the English Longitudinal Study of Ageing (ELSA)\n-View Project"
  },
  {
    "objectID": "projects/index.html#project-1-class-calculator",
    "href": "projects/index.html#project-1-class-calculator",
    "title": "Projects",
    "section": "",
    "text": "Description:\nThis web application was designed to aid in teaching Sociology students about the operationalisation of social class. It was built using R and the Shiny package, and it provides users with teh ability to test how different measure of social class are constructed and used in a survey environment.\n-View Project\n-Source Code"
  },
  {
    "objectID": "projects/index.html#project-2-shiny-conference-appendix-app",
    "href": "projects/index.html#project-2-shiny-conference-appendix-app",
    "title": "Projects",
    "section": "",
    "text": "Description:\nThis web application was designed to aid in conference presentation, it hosts every figure/graph/table in a given paper presentation. The app itself uses R code using the Shiny package.\n-View Project"
  },
  {
    "objectID": "projects/index.html#project-3-remote-survey-hosting-and-structured-data-storage-app",
    "href": "projects/index.html#project-3-remote-survey-hosting-and-structured-data-storage-app",
    "title": "Projects",
    "section": "",
    "text": "Description:\nThis web application was designed to showcase the verstalilty of Shiny, being able to construct surveys and save user inputs using google cloud servers to store in a remote google sheets.\n-View Project"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Here is a list of my courses I have previously taught, or currently teach on:\n\n\nDescription:\nThe course enables students to understand and use multilevel models mainly in the context of social science, but examples are also given from medicine and some aspects of biological science.\nDate: 2025, Semester 2\nCourse Catalogue\n\n\n\n\nDescription:\nThis Year 2 course will introduce students to political data analysis using domestic and international data.\nDate: 2025, Semester 2\nCourse Catalogue\n\n\n\n\nDescription:\nA general introduction to the history of medicine in Western society from the Ancient Greeks to the present.\nDate: 2024, Semester 1\nCourse Catalogue\n\n\n\n\nDescription:\nThis course aims to provide students in the with Quantitative Methods programmes with the mathematical foundations, which will allow them to fully explore advanced methods, as well as gain a full understanding of the mathematic principles behind the basic methods. Throughout the course, the application of mathematics to social science research problems will be emphasised.\nDate: 2024, Semester 1\nCourse Catalogue\n\n\n\n\nDescription:\nThis course is designed to meet the needs of those students who want to develop their knowledge of survey methods, introductory statistics and quantitative data analysis.\nDate: 2024, Semester 2 & 2022, Semester 2\nCourse Catalogue\n\n\n\n\nDescription:\nThe course will introduce students to classic themes and concepts in colonial studies, post-colonial studies, cultural studies and political economy, through accounts of cultural, political and economic change in the social anthropology of Africa, Latin America, South Asia and the Pacific.\nDate: 2023, Semester 2\nCourse Catalogue\n\n\n\n\nDescription:\nThe course explores different sociological understandings of such relationships and debates about the nature of social change in personal life.\nDate: 2023, Semester 2\nCourse Catalogue\n\n\n\n\nDescription:\nThis course introduces some of the key ideas of the discipline by examining the relationship between ‘individuals’ and ‘societies’. Among the topics will be the social nature of the self, the influence of groups, gender identities, nationalism and the city.\nDate: 2023, Semester 1\nCourse Catalogue\n\n\n\n\nDescription:\nThis course is designed to introduce students to a variety of statistical approaches to analyzing longitudinal data. The course will have a practical focus and introduce students to analyzing existing large-scale household panel datasets. These datasets will include the British Household Panel Survey and Understanding Society (the UK Household Longitudinal Survey). The course will be taught using Stata software.\nDate: 2022, Semester 2\nCourse Catalogue"
  },
  {
    "objectID": "teaching/index.html#introduction-to-political-data-analysis-plit08009",
    "href": "teaching/index.html#introduction-to-political-data-analysis-plit08009",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nThis Year 2 course will introduce students to political data analysis using domestic and international data.\nDate: 2025, Semester 2\nCourse Catalogue"
  },
  {
    "objectID": "teaching/index.html#history-of-western-medicine-stis08009",
    "href": "teaching/index.html#history-of-western-medicine-stis08009",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nA general introduction to the history of medicine in Western society from the Ancient Greeks to the present.\nDate: 2024, Semester 1\nCourse Catalogue"
  },
  {
    "objectID": "teaching/index.html#mathematics-for-social-science-ssps08009",
    "href": "teaching/index.html#mathematics-for-social-science-ssps08009",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nThis course aims to provide students in the with Quantitative Methods programmes with the mathematical foundations, which will allow them to fully explore advanced methods, as well as gain a full understanding of the mathematic principles behind the basic methods. Throughout the course, the application of mathematics to social science research problems will be emphasised.\nDate: 2024, Semester 1\nCourse Catalogue"
  },
  {
    "objectID": "teaching/index.html#doing-survey-research-scil10063",
    "href": "teaching/index.html#doing-survey-research-scil10063",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nThis course is designed to meet the needs of those students who want to develop their knowledge of survey methods, introductory statistics and quantitative data analysis.\nDate: 2024, Semester 2 & 2022, Semester 2\nCourse Catalogue"
  },
  {
    "objectID": "teaching/index.html#empires-scan08010",
    "href": "teaching/index.html#empires-scan08010",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nThe course will introduce students to classic themes and concepts in colonial studies, post-colonial studies, cultural studies and political economy, through accounts of cultural, political and economic change in the social anthropology of Africa, Latin America, South Asia and the Pacific.\nDate: 2023, Semester 2\nCourse Catalogue"
  },
  {
    "objectID": "teaching/index.html#intimate-relationships-scil10023",
    "href": "teaching/index.html#intimate-relationships-scil10023",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nThe course explores different sociological understandings of such relationships and debates about the nature of social change in personal life.\nDate: 2023, Semester 2\nCourse Catalogue"
  },
  {
    "objectID": "teaching/index.html#sociology-1a-the-sociological-imagination-individuals-and-society-scil08004",
    "href": "teaching/index.html#sociology-1a-the-sociological-imagination-individuals-and-society-scil08004",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nThis course introduces some of the key ideas of the discipline by examining the relationship between ‘individuals’ and ‘societies’. Among the topics will be the social nature of the self, the influence of groups, gender identities, nationalism and the city.\nDate: 2023, Semester 1\nCourse Catalogue"
  },
  {
    "objectID": "teaching/index.html#longitudinal-data-analysis-pgsp11487",
    "href": "teaching/index.html#longitudinal-data-analysis-pgsp11487",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nThis course is designed to introduce students to a variety of statistical approaches to analyzing longitudinal data. The course will have a practical focus and introduce students to analyzing existing large-scale household panel datasets. These datasets will include the British Household Panel Survey and Understanding Society (the UK Household Longitudinal Survey). The course will be taught using Stata software.\nDate: 2022, Semester 2\nCourse Catalogue"
  },
  {
    "objectID": "presentations/index.html#the-forgotten-form-of-stratification-sexual-orientation-in-large-social-survey-research",
    "href": "presentations/index.html#the-forgotten-form-of-stratification-sexual-orientation-in-large-social-survey-research",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Stirling Social Stratification Conference\nDate: 2025\nLocation: Stirling, Scotland\n\nView Slides"
  },
  {
    "objectID": "presentations/index.html#weberian-social-status-reimagined-a-sociological-and-empirical-critique-of-existing-status-measures-and-a-viable-alternative",
    "href": "presentations/index.html#weberian-social-status-reimagined-a-sociological-and-empirical-critique-of-existing-status-measures-and-a-viable-alternative",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Stirling All Hands Seminar\nDate: 2025\nLocation: Stirling, Scotland\n\nView Slides"
  },
  {
    "objectID": "presentations/index.html#is-there-really-a-pot-of-gold-at-the-end-of-the-rainbow-an-investigation-of-sexual-orientation-pay-gaps-in-the-uk",
    "href": "presentations/index.html#is-there-really-a-pot-of-gold-at-the-end-of-the-rainbow-an-investigation-of-sexual-orientation-pay-gaps-in-the-uk",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Large Social Survey Symposium\nDate: 2025\nLocation: Edinburgh, Scotland\n\nView Slides"
  },
  {
    "objectID": "presentations/index.html#gender-performance-scale-as-a-quantitative-insight-into-intracategorical-intersectionality",
    "href": "presentations/index.html#gender-performance-scale-as-a-quantitative-insight-into-intracategorical-intersectionality",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Intersectional analysis and quantitative methods\nDate: 2025\nLocation: Loughborough, England\n\nView Slides\nView Jupyter Notebook File"
  },
  {
    "objectID": "presentations/index.html#a-simulation-study-comparing-handling-missing-data-strategies",
    "href": "presentations/index.html#a-simulation-study-comparing-handling-missing-data-strategies",
    "title": "Presentations",
    "section": "",
    "text": "Conference: New Directions\nDate: 2025\nLocation: Edinburgh, Scotland\n\nView Slides\nView Github\nView Jupyter Notebook File of Simulation\nView Jupyter Notebook File of Handling Missing Data in different Software"
  },
  {
    "objectID": "presentations/index.html#weberian-social-status-reimagined-a-sociological-and-empirical-critique-of-existing-status-measures-and-a-viable-alternative-1",
    "href": "presentations/index.html#weberian-social-status-reimagined-a-sociological-and-empirical-critique-of-existing-status-measures-and-a-viable-alternative-1",
    "title": "Presentations",
    "section": "",
    "text": "Conference: Large Social Survey Symposium\nDate: 2024\nLocation: Edinburgh, Scotland\n\nView Slides\nView .do File\nView Jupyter Notebook File"
  },
  {
    "objectID": "projects/index.html#project-4-socstrat-a-social-stratification-command-in-stata",
    "href": "projects/index.html#project-4-socstrat-a-social-stratification-command-in-stata",
    "title": "Projects",
    "section": "",
    "text": "Description:\nStata program to compute and analyze social stratification variables. socstrat is designed to calculate and analyze social stratification variables based on two variables: the National Statistics Socio-Economic Classification (NS-SEC) or Registrar General’s Social Class (RGSC). The command requires at least two variables to define the standard occupation classification (SOC) code and the employment status (via the socstatus() option and generates a new variable that represents the computed social stratification variable.\n-View Project"
  },
  {
    "objectID": "projects/index.html#project-5-dervied-detailed-ethnicity-measure-for-english-longitudinal-study-of-ageing-elsa",
    "href": "projects/index.html#project-5-dervied-detailed-ethnicity-measure-for-english-longitudinal-study-of-ageing-elsa",
    "title": "Projects",
    "section": "",
    "text": "Description:\nDetailed Roadmap of Ethnicity Data Collection in the English Longitudinal Study of Ageing (ELSA)\n-View Project"
  },
  {
    "objectID": "presentations/Handling_Missing_Data_Methods_Software.html",
    "href": "presentations/Handling_Missing_Data_Methods_Software.html",
    "title": "Dr Scott Oatley",
    "section": "",
    "text": "Before any analysis. Both the R kernal and Stata kernal need to be properly setup. First start with R. We first need to install the correct Python to R package using pip.\n\npip install rpy2\n\nRequirement already satisfied: rpy2 in /opt/anaconda3/lib/python3.12/site-packages (3.5.17)\nRequirement already satisfied: cffi&gt;=1.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from rpy2) (1.17.1)\nRequirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from rpy2) (3.1.4)\nRequirement already satisfied: tzlocal in /opt/anaconda3/lib/python3.12/site-packages (from rpy2) (5.3.1)\nRequirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi&gt;=1.15.1-&gt;rpy2) (2.21)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2-&gt;rpy2) (2.1.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\nAfter we have done this we then need to setup R within the Python kernal\n\n%load_ext rpy2.ipython\n\nFinally a quick little test to make sure R now works. Anytime we want R code to run we have to use the magic cells %%R.\n\n%%R\nx &lt;- c(1,2,3,4,5)\nmean(x)\n\n[1] 3\n\n\nNow we have to install the python to stata dependecies to get Stata to work in a Python kernal. We start by installing the pystata package.\n\npip install pystata\n\nRequirement already satisfied: pystata in /opt/anaconda3/lib/python3.12/site-packages (0.0.1)\nRequirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from pystata) (2.2.2)\nRequirement already satisfied: numpy&gt;=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;pystata) (1.26.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;pystata) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;pystata) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;pystata) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;pystata) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\nWe then also install the stata_setup package.\n\npip install --upgrade --user stata_setup\n\nRequirement already satisfied: stata_setup in /Users/scottoatley/.local/lib/python3.12/site-packages (0.1.3)\nRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from stata_setup) (1.26.4)\nRequirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from stata_setup) (2.2.2)\nRequirement already satisfied: ipython in /opt/anaconda3/lib/python3.12/site-packages (from stata_setup) (8.27.0)\nRequirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython-&gt;stata_setup) (5.1.1)\nRequirement already satisfied: jedi&gt;=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython-&gt;stata_setup) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython-&gt;stata_setup) (0.1.6)\nRequirement already satisfied: prompt-toolkit&lt;3.1.0,&gt;=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython-&gt;stata_setup) (3.0.43)\nRequirement already satisfied: pygments&gt;=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython-&gt;stata_setup) (2.15.1)\nRequirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython-&gt;stata_setup) (0.2.0)\nRequirement already satisfied: traitlets&gt;=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython-&gt;stata_setup) (5.14.3)\nRequirement already satisfied: pexpect&gt;4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython-&gt;stata_setup) (4.8.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;stata_setup) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;stata_setup) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;stata_setup) (2023.3)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi&gt;=0.16-&gt;ipython-&gt;stata_setup) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect&gt;4.3-&gt;ipython-&gt;stata_setup) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython-&gt;stata_setup) (0.2.5)\nRequirement already satisfied: six&gt;=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;stata_setup) (1.16.0)\nRequirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data-&gt;ipython-&gt;stata_setup) (0.8.3)\nRequirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data-&gt;ipython-&gt;stata_setup) (2.0.5)\nRequirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data-&gt;ipython-&gt;stata_setup) (0.2.2)\nNote: you may need to restart the kernel to use updated packages.\n\n\nWe now import that stata_setup package into the python kernal.\n\nimport stata_setup\n\nAnd finally we import Stata itself into the kernal.\n\nstata_setup.config(\"/Applications/Stata\", \"se\")\n\n\n  ___  ____  ____  ____  ____ ®\n /__    /   ____/   /   ____/      18.0\n___/   /   /___/   /   /___/       SE—Standard Edition\n\n Statistics and Data Science       Copyright 1985-2023 StataCorp LLC\n                                   StataCorp\n                                   4905 Lakeway Drive\n                                   College Station, Texas 77845 USA\n                                   800-STATA-PC        https://www.stata.com\n                                   979-696-4600        stata@stata.com\n\nStata license: Unlimited-user network, expiring 14 Sep 2025\nSerial number: 401809305318\n  Licensed to: Scott oatley\n               University of Edinburgh\n\nNotes:\n      1. Unicode is supported; see help unicode_advice.\n      2. Maximum number of variables is set to 5,000 but can be increased;\n          see help set_maxvar.\n\n\nAnd we now test that Stata works by doing a simple test using display.\n\n%%stata\n\ndisplay 1+1\n\n\n. \n. display 1+1\n2\n\n. \n\n\nAnd now back to R to make sure they both can run at the same time.\n\n%%R\ny &lt;- c(1,2,3,NA)\nis.na(y) # returns a vector (F F F T)\n\n[1] FALSE FALSE FALSE  TRUE\n\n\nEach handling missing data method will now be explored using the four aforementioned software programs: R, Stata, Python, and Mplus. Some methods are not possible with certain software, when this happens, only relevant software will be mentioned. We start with a simple simulation of an OLS linear regression model with three continuous independent variables that are normally distributed with means ranging from 40-200 and standard deviations ranging from 5-50. First we start with R, then Stata, then Python, and finally Mplus.\nAll methods will be produced.\n\n%%R\n\n# Load necessary library\nset.seed(123)  # Ensure reproducibility\n\n# Generate metric independent variables\nx1 &lt;- rnorm(1000, mean = 40, sd = 12)\nx2 &lt;- rnorm(1000, mean = 200, sd = 50)\nx3 &lt;- rnorm(1000, mean = 150, sd = 5)\n\n# Generate metric dependent variable\ny &lt;- 30*x1 + 40*x2 + 50*x3 + rnorm(1000, mean = 5000, sd = 1500)\n\n# Create data frame\ndata &lt;- data.frame(y, x1, x2, x3)\n\n# Run regression\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4584.3  -950.2     0.7  1012.9  4338.4 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2726.9512  1462.4421   1.865   0.0625 .  \nx1            29.8375     3.9740   7.508 1.33e-13 ***\nx2            39.7564     0.9369  42.432  &lt; 2e-16 ***\nx3            65.4442     9.6348   6.792 1.89e-11 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1489 on 996 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6652 \nF-statistic: 662.5 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\nThis is the same as this in Stata:\n\n%%stata\n\n* Set seed\nset seed 123\n\n* Set number of observations\nset obs 1000\n\n* Generate metric independent variable #1\ndrawnorm x1, n(1000) means(40) sds(12)\n\n* Generate metric independent variable #2\ndrawnorm x2, n(1000) means(200) sds(50)\n\n* Generate metric independent variable #3\ndrawnorm x3, n(1000) means(150) sds(5)\n\n* Generate metric dependent variable with large coefficients\ngen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n* Run Model\nregress y x1 x2 x3\n\netable\n\ntwoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\") legend(off)\n\n\n. \n. * Set seed\n. set seed 123\n\n. \n. * Set number of observations\n. set obs 1000\nNumber of observations (_N) was 0, now 1,000.\n\n. \n. * Generate metric independent variable #1\n. drawnorm x1, n(1000) means(40) sds(12)\n\n. \n. * Generate metric independent variable #2\n. drawnorm x2, n(1000) means(200) sds(50)\n\n. \n. * Generate metric independent variable #3\n. drawnorm x3, n(1000) means(150) sds(5)\n\n. \n. * Generate metric dependent variable with large coefficients\n. gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n. \n. * Run Model\n. regress y x1 x2 x3\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(3, 996)       =    714.04\n       Model |  4.7321e+09         3  1.5774e+09   Prob &gt; F        =    0.0000\n    Residual |  2.2002e+09       996  2209043.89   R-squared       =    0.6826\n-------------+----------------------------------   Adj R-squared   =    0.6817\n       Total |  6.9323e+09       999  6939202.43   Root MSE        =    1486.3\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |    29.4092   4.032441     7.29   0.000     21.49614    37.32225\n          x2 |   42.71715   .9441681    45.24   0.000     40.86437    44.56994\n          x3 |   54.51955   9.245538     5.90   0.000     36.37658    72.66252\n       _cons |   3778.896   1402.667     2.69   0.007     1026.375    6531.417\n------------------------------------------------------------------------------\n\n. \n. etable\n\n---------------------------------\n                            y    \n---------------------------------\nx1                         29.409\n                          (4.032)\nx2                         42.717\n                          (0.944)\nx3                         54.520\n                          (9.246)\nIntercept                3778.896\n                       (1402.667)\nNumber of observations       1000\n---------------------------------\n\n. \n. twoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\"\n&gt; ) legend(off)\n\n. \n\n\n\n\n\n\n\n\n\nNow don’t worry too much that the results from the models aren’t exactly perfect. That is because we have different seeds for each software. Substantively they are the same and that is all that matters here. Now for Python.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Generate metric independent variables\nx1 = np.random.normal(loc=40, scale=12, size=1000)\nx2 = np.random.normal(loc=200, scale=50, size=1000)\nx3 = np.random.normal(loc=150, scale=5, size=1000)\n\n# Generate metric dependent variable\ny = 30*x1 + 40*x2 + 50*x3 + np.random.normal(loc=5000, scale=1500, size=1000)\n\n# Create DataFrame\ndf = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n\n# Run regression\nX = df[['x1', 'x2', 'x3']]  # Independent variables\nX = sm.add_constant(X)  # Add intercept\nmodel = sm.OLS(df['y'], X).fit()  # Fit model\n\n# Display summary\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.618\nModel:                            OLS   Adj. R-squared:                  0.617\nMethod:                 Least Squares   F-statistic:                     537.7\nDate:                Sun, 06 Apr 2025   Prob (F-statistic):          1.03e-207\nTime:                        13:35:34   Log-Likelihood:                -8719.9\nNo. Observations:                1000   AIC:                         1.745e+04\nDf Residuals:                     996   BIC:                         1.747e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5257.2104   1467.373      3.583      0.000    2377.713    8136.707\nx1            26.4715      3.914      6.764      0.000      18.792      34.151\nx2            38.7536      0.981     39.513      0.000      36.829      40.678\nx3            51.5446      9.574      5.384      0.000      32.757      70.332\n==============================================================================\nOmnibus:                        2.499   Durbin-Watson:                   1.897\nProb(Omnibus):                  0.287   Jarque-Bera (JB):                2.519\nSkew:                           0.122   Prob(JB):                        0.284\nKurtosis:                       2.964   Cond. No.                     8.01e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 8.01e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nAnd finally, as Mplus as written through R. Currently, no access to full MPlus licence which does cause some issues with getting MPlus to run properly. For now this is all the MPlus I can produce.\n\n%%R\n\ninstall.packages(\"MplusAutomation\")\ninstall.packages(\"texreg\")\n\n--- Please select a CRAN mirror for use in this session ---\nSecure CRAN mirrors \n\n 1: 0-Cloud [https]\n 2: Australia (Canberra) [https]\n 3: Australia (Melbourne 1) [https]\n 4: Australia (Melbourne 2) [https]\n 5: Austria (Wien 1) [https]\n 6: Belgium (Brussels) [https]\n 7: Brazil (PR) [https]\n 8: Brazil (SP 1) [https]\n 9: Brazil (SP 2) [https]\n10: Bulgaria [https]\n11: Canada (ON 1) [https]\n12: Canada (ON 2) [https]\n13: Chile (Santiago) [https]\n14: China (Beijing 1) [https]\n15: China (Beijing 2) [https]\n16: China (Beijing 3) [https]\n17: China (Hefei) [https]\n18: China (Hong Kong) [https]\n19: China (Jinan) [https]\n20: China (Lanzhou) [https]\n21: China (Nanjing) [https]\n22: China (Shanghai 2) [https]\n23: China (Shenzhen) [https]\n24: China (Wuhan) [https]\n25: Colombia (Cali) [https]\n26: Costa Rica [https]\n27: Cyprus [https]\n28: Czech Republic [https]\n29: Denmark [https]\n30: East Asia [https]\n31: Ecuador (Cuenca) [https]\n32: Finland (Helsinki) [https]\n33: France (Lyon 1) [https]\n34: France (Lyon 2) [https]\n35: France (Paris 1) [https]\n36: Germany (Erlangen) [https]\n37: Germany (Göttingen) [https]\n38: Germany (Leipzig) [https]\n39: Germany (Münster) [https]\n40: Greece [https]\n41: Hungary [https]\n42: Iceland [https]\n43: India (Bengaluru) [https]\n44: India (Bhubaneswar) [https]\n45: Indonesia (Banda Aceh) [https]\n46: Iran (Mashhad) [https]\n47: Italy (Milano) [https]\n48: Italy (Padua) [https]\n49: Japan (Yonezawa) [https]\n50: Korea (Gyeongsan-si) [https]\n51: Mexico (Mexico City) [https]\n52: Mexico (Texcoco) [https]\n53: Morocco [https]\n54: Netherlands (Dronten) [https]\n55: New Zealand [https]\n56: Norway [https]\n57: Poland [https]\n58: South Africa (Johannesburg) [https]\n59: Spain (A Coruña) [https]\n60: Spain (Madrid) [https]\n61: Sweden (Umeå) [https]\n62: Switzerland (Zurich 1) [https]\n63: Taiwan (Taipei) [https]\n64: Turkey (Denizli) [https]\n65: UK (Bristol) [https]\n66: UK (London 1) [https]\n67: USA (IA) [https]\n68: USA (MI) [https]\n69: USA (MO) [https]\n70: USA (OH) [https]\n71: USA (OR) [https]\n72: USA (PA 1) [https]\n73: USA (TN) [https]\n74: USA (UT) [https]\n75: United Arab Emirates [https]\n76: Uruguay [https]\n77: (other mirrors)\n\n\nThe downloaded binary packages are in\n    /var/folders/5m/wtp81d3x2vx4v3kw6v37qj180000gn/T//RtmprBoE5j/downloaded_packages\n\nThe downloaded binary packages are in\n    /var/folders/5m/wtp81d3x2vx4v3kw6v37qj180000gn/T//RtmprBoE5j/downloaded_packages\n\n\nSelection:  67\n\n\ntrying URL 'https://mirror.las.iastate.edu/CRAN/bin/macosx/big-sur-arm64/contrib/4.4/MplusAutomation_1.1.1.tgz'\nContent type 'application/x-gzip' length 2769853 bytes (2.6 MB)\n==================================================\ndownloaded 2.6 MB\n\ntrying URL 'https://mirror.las.iastate.edu/CRAN/bin/macosx/big-sur-arm64/contrib/4.4/texreg_1.39.4.tgz'\nContent type 'application/x-gzip' length 1340319 bytes (1.3 MB)\n==================================================\ndownloaded 1.3 MB\n\nIn addition: Warning message:\nIn doTryCatch(return(expr), name, parentenv, handler) :\n  unable to load shared object '/Library/Frameworks/R.framework/Resources/modules//R_X11.so':\n  dlopen(/Library/Frameworks/R.framework/Resources/modules//R_X11.so, 0x0006): Library not loaded: /opt/X11/lib/libSM.6.dylib\n  Referenced from: &lt;31EADEB5-0A17-3546-9944-9B3747071FE8&gt; /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/modules/R_X11.so\n  Reason: tried: '/opt/X11/lib/libSM.6.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/X11/lib/libSM.6.dylib' (no such file), '/opt/X11/lib/libSM.6.dylib' (no such file), '/usr/local/lib/libSM.6.dylib' (no such file), '/usr/lib/libSM.6.dylib' (no such file, not in dyld cache)\n\n\n\n%%R\n\n# Load necessary libraries\nlibrary(MplusAutomation)\nlibrary(texreg)\n\n# Define the OLS linear regression model\nolsModel &lt;- mplusObject(\n  TITLE = \"OLS Linear Regression Model in Mplus;\",\n  MODEL = \"\n     mpg ON hp wt;\",  # mpg as dependent variable, hp and wt as independent variables\n  OUTPUT = \"CINTERVAL;\",  # Request confidence intervals for estimates\n  rdata = mtcars  # Use the mtcars dataset\n)\n\n# Run the model\nfit_ols &lt;- mplusModeler(olsModel, modelout = \"ols_model.inp\", run = 1L)\n\n# Display fit statistics\nscreenreg(fit_ols, summaries = c(\"Observations\", \"CFI\", \"SRMR\"), single.row = TRUE)\n\n\n==================================\n                  TITLE           \n----------------------------------\n MPG&lt;-HP          -0.03 (0.01) ***\n MPG&lt;-WT          -3.88 (0.60) ***\n MPG&lt;-Intercepts  37.23 (1.52) ***\n MPG&lt;-&gt;MPG         6.10 (1.52) ***\n----------------------------------\nObservations      32              \nCFI                1.00           \nSRMR               0.00           \n==================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n FALSE\n\n\nVersion:  1.1.1\nWe work hard to write this free software. Please help us get credit by citing: \n\nHallquist, M. N. & Wiley, J. F. (2018). MplusAutomation: An R Package for Facilitating Large-Scale Latent Variable Analyses in Mplus. Structural Equation Modeling, 25, 621-638. doi: 10.1080/10705511.2017.1402334.\n\n-- see citation(\"MplusAutomation\").\nVersion:  1.39.4\nDate:     2024-07-23\nAuthor:   Philip Leifeld (University of Manchester)\n\nConsider submitting praise using the praise or praise_interactive functions.\nPlease cite the JSS article in your publications -- see citation(\"texreg\").\n\n\nThe simple OLS linear regression model has now been conducted. We now perform the same model under a sem framework.\n\n%%stata\n\nclear\n\n* Set seed\nset seed 123\n\n* Set number of observations\nset obs 1000\n\n* Generate metric independent variable #1\ndrawnorm x1, n(1000) means(40) sds(12)\n\n* Generate metric independent variable #2\ndrawnorm x2, n(1000) means(200) sds(50)\n\n* Generate metric independent variable #3\ndrawnorm x3, n(1000) means(150) sds(5)\n\n* Generate metric dependent variable with large coefficients\ngen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\nsem (y &lt;- x1 x2 x3), method(mlmv)\n\netable, append \n\n\n. \n. clear\n\n. \n. * Set seed\n. set seed 123\n\n. \n. * Set number of observations\n. set obs 1000\nNumber of observations (_N) was 0, now 1,000.\n\n. \n. * Generate metric independent variable #1\n. drawnorm x1, n(1000) means(40) sds(12)\n\n. \n. * Generate metric independent variable #2\n. drawnorm x2, n(1000) means(200) sds(50)\n\n. \n. * Generate metric independent variable #3\n. drawnorm x3, n(1000) means(150) sds(5)\n\n. \n. * Generate metric dependent variable with large coefficients\n. gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n. \n. sem (y &lt;- x1 x2 x3), method(mlmv)\n\nEndogenous variables\n  Observed: y\n\nExogenous variables\n  Observed: x1 x2 x3\n\nFitting target model:\nIteration 0:  Log likelihood = -20967.462  \nIteration 1:  Log likelihood = -20967.462  \n\nStructural equation model                                Number of obs = 1,000\nEstimation method: mlmv\n\nLog likelihood = -20967.462\n\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  y          |\n          x1 |    29.4092   4.024368     7.31   0.000     21.52158    37.29681\n          x2 |   42.71715   .9422779    45.33   0.000     40.87032    44.56398\n          x3 |   54.51955   9.227029     5.91   0.000     36.43491     72.6042\n       _cons |   3778.896   1399.858     2.70   0.007     1035.223    6522.568\n-------------+----------------------------------------------------------------\n     var(e.y)|    2200208   98396.28                       2015565     2401765\n------------------------------------------------------------------------------\nLR test of model vs. saturated: chi2(0) = 0.00                 Prob &gt; chi2 = .\n\n. \n. etable, append \n\n---------------------------------------------\n                            y          2     \n---------------------------------------------\nx1                         29.409      29.409\n                          (4.032)     (4.024)\nx2                         42.717      42.717\n                          (0.944)     (0.942)\nx3                         54.520      54.520\n                          (9.246)     (9.227)\nIntercept                3778.896    3778.896\n                       (1402.667)  (1399.858)\nvar(e.y)                             2.20e+06\n                                  (98396.280)\nNumber of observations       1000        1000\n---------------------------------------------\n\n. \n\n\n\n%%R\n\n# Load necessary libraries\nlibrary(lavaan)\nlibrary(MASS)  # For mvrnorm (multivariate normal distribution)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate metric independent variables\nx1 &lt;- rnorm(1000, mean = 40, sd = 12)\nx2 &lt;- rnorm(1000, mean = 200, sd = 50)\nx3 &lt;- rnorm(1000, mean = 150, sd = 5)\n\n# Generate metric dependent variable\ny &lt;- 30*x1 + 40*x2 + 50*x3 + rnorm(1000, mean = 5000, sd = 1500)\n\n# Create a dataframe\ndata &lt;- data.frame(y, x1, x2, x3)\n\n# Define the SEM model\nsem_model &lt;- '\n  y ~ x1 + x2 + x3\n'\n\n# Fit the model using Maximum Likelihood with Missing Values (MLMV)\nfit &lt;- sem(sem_model, data = data, missing = \"ml\")\n\n# Print model summary\nsummary(fit, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                          1000\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                              1097.097\n  Degrees of freedom                                 3\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8722.665\n  Loglikelihood unrestricted model (H1)      -8722.665\n                                                      \n  Akaike (AIC)                               17455.330\n  Bayesian (BIC)                             17479.869\n  Sample-size adjusted Bayesian (SABIC)      17463.989\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050                NA\n  P-value H_0: Robust RMSEA &gt;= 0.080                NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nRegressions:\n                   Estimate     Std.Err   z-value  P(&gt;|z|)   Std.lv     Std.all\n  y ~                                                                          \n    x1                  29.837     3.966    7.523    0.000      29.837    0.138\n    x2                  39.756     0.935   42.518    0.000      39.756    0.780\n    x3                  65.444     9.616    6.806    0.000      65.444    0.124\n\nIntercepts:\n                   Estimate     Std.Err   z-value  P(&gt;|z|)   Std.lv     Std.all\n   .y                 2726.951  1459.514    1.868    0.062    2726.951    1.060\n\nVariances:\n                   Estimate     Std.Err   z-value  P(&gt;|z|)   Std.lv     Std.all\n   .y              2207681.041 98745.450   22.357    0.000 2207681.041    0.334\n\n\n\nThis is lavaan 0.6-18\nlavaan is FREE software! Please report any bugs.\nIn addition: Warning messages:\n1: lavaan-&gt;lav_data_full():  \n   some observed variances are larger than 1000000 use varTable(fit) to \n   investigate \n2: lavaan-&gt;lavData():  \n   some observed variances in the sample covariance matrix are larger than \n   1000000. \n3: lavaan-&gt;lavData():  \n   some observed variances in the sample covariance matrix are larger than \n   1000000. \n4: lavaan-&gt;lavData():  \n   some observed variances in the sample covariance matrix are larger than \n   1000000. \n5: lavaan-&gt;lavData():  \n   some observed variances in the sample covariance matrix are larger than \n   1000000. \n\n\n\npip install semopy\n\nRequirement already satisfied: semopy in /opt/anaconda3/lib/python3.12/site-packages (2.3.11)\nRequirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from semopy) (1.13.1)\nRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from semopy) (1.26.4)\nRequirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from semopy) (2.2.2)\nRequirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from semopy) (1.13.2)\nRequirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from semopy) (1.5.1)\nRequirement already satisfied: statsmodels in /opt/anaconda3/lib/python3.12/site-packages (from semopy) (0.14.2)\nRequirement already satisfied: numdifftools in /opt/anaconda3/lib/python3.12/site-packages (from semopy) (0.9.41)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;semopy) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;semopy) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas-&gt;semopy) (2023.3)\nRequirement already satisfied: joblib&gt;=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn-&gt;semopy) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn-&gt;semopy) (3.5.0)\nRequirement already satisfied: patsy&gt;=0.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels-&gt;semopy) (0.5.6)\nRequirement already satisfied: packaging&gt;=21.3 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels-&gt;semopy) (24.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy-&gt;semopy) (1.3.0)\nRequirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from patsy&gt;=0.5.6-&gt;statsmodels-&gt;semopy) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom semopy import Model\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# Generate metric independent variables\nx1 = np.random.normal(loc=40, scale=12, size=1000)\nx2 = np.random.normal(loc=200, scale=50, size=1000)\nx3 = np.random.normal(loc=150, scale=5, size=1000)\n\n# Generate metric dependent variable\ny = 30*x1 + 40*x2 + 50*x3 + np.random.normal(loc=5000, scale=1500, size=1000)\n\n# Create a DataFrame\ndata = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n\n# Define the SEM model in semopy syntax\nsem_model = \"\"\"\ny ~ x1 + x2 + x3\n\"\"\"\n\n# Create and fit the model\nmodel = Model(sem_model)\nmodel.fit(data)\n\n# Print model results\nprint(model.inspect())\n\n  lval  op rval      Estimate       Std. Err    z-value       p-value\n0    y   ~   x1  2.826314e+01       4.470083   6.322734  2.569751e-10\n1    y   ~   x2  3.870783e+01       1.120236  34.553275  0.000000e+00\n2    y   ~   x3  9.507102e+00      10.935110   0.869411  3.846225e-01\n3    y  ~~    y  2.875963e+06  128616.985180  22.360680  0.000000e+00\n\n\n\n%%R\n\n# Load necessary libraries\nlibrary(MplusAutomation)\nlibrary(texreg)\n\n# Define the SEM model\nsemModel &lt;- mplusObject(\n  TITLE = \"Structural Equation Model (SEM) in Mplus;\",\n  MODEL = \"\n     mpg ON hp wt;\",  # Regression model in SEM form\n  OUTPUT = \"STANDARDIZED CINTERVAL;\",  # Request standardized results and confidence intervals\n  rdata = mtcars  # Use the mtcars dataset\n)\n\n# Run the SEM model in Mplus\nfit_sem &lt;- mplusModeler(\n  semModel, \n  modelout = \"sem_model.inp\",  # Output .inp file\n  run = 1L                     # Run the model after writing input\n)\n\n# Display model fit statistics\nscreenreg(fit_sem, summaries = c(\"Observations\", \"CFI\", \"SRMR\"), single.row = TRUE)\n\n\n==================================\n                  TITLE           \n----------------------------------\n MPG&lt;-HP          -0.03 (0.01) ***\n MPG&lt;-WT          -3.88 (0.60) ***\n MPG&lt;-Intercepts  37.23 (1.52) ***\n MPG&lt;-&gt;MPG         6.10 (1.52) ***\n----------------------------------\nObservations      32              \nCFI                1.00           \nSRMR               0.00           \n==================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n FALSE\n\n\nNow we move on to a simple MCAR model. Starting with Stata.\n\n%%stata\n\nclear\n\n* Set seed\nset seed 123\n\n* Set number of observations\nset obs 1000\n\n* Generate metric independent variable #1\ndrawnorm x1, n(1000) means(40) sds(12)\n\n* Generate metric independent variable #2\ndrawnorm x2, n(1000) means(200) sds(50)\n\n* Generate metric independent variable #3\ndrawnorm x3, n(1000) means(150) sds(5)\n\n* Generate metric dependent variable with large coefficients\ngen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n* Generate MCAR \ngen rmcar = rbinomial(1, 0.5)  // MCAR: 50% chance of missingness (binary random)\nreplace x2 = . if rmcar == 0  // Set x to missing where rmcar == 0\n\nregress y x1 x2 x3\n\netable, append\n\ntwoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\") legend(off)\n\n\n\n. \n. clear\n\n. \n. * Set seed\n. set seed 123\n\n. \n. * Set number of observations\n. set obs 1000\nNumber of observations (_N) was 0, now 1,000.\n\n. \n. * Generate metric independent variable #1\n. drawnorm x1, n(1000) means(40) sds(12)\n\n. \n. * Generate metric independent variable #2\n. drawnorm x2, n(1000) means(200) sds(50)\n\n. \n. * Generate metric independent variable #3\n. drawnorm x3, n(1000) means(150) sds(5)\n\n. \n. * Generate metric dependent variable with large coefficients\n. gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n. \n. * Generate MCAR \n. gen rmcar = rbinomial(1, 0.5)  // MCAR: 50% chance of missingness (binary ran\n&gt; dom)\n\n. replace x2 = . if rmcar == 0  // Set x to missing where rmcar == 0\n(492 real changes made, 492 to missing)\n\n. \n. regress y x1 x2 x3\n\n      Source |       SS           df       MS      Number of obs   =       508\n-------------+----------------------------------   F(3, 504)       =    325.45\n       Model |  2.3023e+09         3   767444127   Prob &gt; F        =    0.0000\n    Residual |  1.1885e+09       504  2358099.32   R-squared       =    0.6595\n-------------+----------------------------------   Adj R-squared   =    0.6575\n       Total |  3.4908e+09       507  6885235.58   Root MSE        =    1535.6\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   25.21384    5.94201     4.24   0.000     13.53968      36.888\n          x2 |   42.55748   1.384992    30.73   0.000     39.83641    45.27855\n          x3 |   48.17947   13.13299     3.67   0.000     22.37733    73.98162\n       _cons |   4912.711   2007.467     2.45   0.015     968.6771    8856.744\n------------------------------------------------------------------------------\n\n. \n. etable, append\n\n--------------------------------------------------------\n                            y          2           y    \n--------------------------------------------------------\nx1                         29.409      29.409     25.214\n                          (4.032)     (4.024)    (5.942)\nx2                         42.717      42.717     42.557\n                          (0.944)     (0.942)    (1.385)\nx3                         54.520      54.520     48.179\n                          (9.246)     (9.227)   (13.133)\nIntercept                3778.896    3778.896   4912.711\n                       (1402.667)  (1399.858) (2007.467)\nvar(e.y)                             2.20e+06           \n                                  (98396.280)           \nNumber of observations       1000        1000        508\n--------------------------------------------------------\n\n. \n. twoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\"\n&gt; ) legend(off)\n\n. \n. \n\n\n\n\n\n\n\n\n\n\n%%R\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate metric independent variables\nx1 &lt;- rnorm(1000, mean = 40, sd = 12)\nx2 &lt;- rnorm(1000, mean = 200, sd = 50)\nx3 &lt;- rnorm(1000, mean = 150, sd = 5)\n\n# Generate metric dependent variable\ny &lt;- 30*x1 + 40*x2 + 50*x3 + rnorm(1000, mean = 5000, sd = 1500)\n\n# Introduce 50% Missing Completely at Random (MCAR) in x2\nrmcar &lt;- rbinom(1000, 1, 0.5)  # Binary indicator: 50% missingness\nx2[rmcar == 0] &lt;- NA        # Set x2 to NA for half the observations\n\n# Create data frame\ndata &lt;- data.frame(y, x1, x2, x3)\n\n# Run OLS regression (handling missing data with listwise deletion by default)\nols_model &lt;- lm(y ~ x1 + x2 + x3, data = data)\n\n# Display regression results\nsummary(ols_model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4464.9 -1057.0   -31.6  1042.2  4294.4 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  386.332   2177.578   0.177    0.859    \nx1            29.499      5.908   4.993 8.28e-07 ***\nx2            40.483      1.364  29.671  &lt; 2e-16 ***\nx3            80.047     14.416   5.553 4.62e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1548 on 489 degrees of freedom\n  (507 observations deleted due to missingness)\nMultiple R-squared:  0.6697,    Adjusted R-squared:  0.6677 \nF-statistic: 330.5 on 3 and 489 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Set number of observations\nn = 1000\n\n# Generate independent variables\nx1 = np.random.normal(loc=40, scale=12, size=n)\nx2 = np.random.normal(loc=200, scale=50, size=n)\nx3 = np.random.normal(loc=150, scale=5, size=n)\n\n# Generate dependent variable\ny = 30*x1 + 40*x2 + 50*x3 + np.random.normal(loc=5000, scale=1500, size=n)\n\n# Introduce 50% Missing Completely at Random (MCAR) in x2\nrmcar = np.random.binomial(1, 0.5, size=n)  # Binary indicator: 50% missingness\nx2[rmcar == 0] = np.nan  # Set x2 to NaN for half the observations\n\n# Create DataFrame\ndata = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n\n# Drop rows with missing values (listwise deletion) and run OLS regression\ndata_complete = data.dropna()  # Remove rows with NaN\nX = data_complete[['x1', 'x2', 'x3']]\nX = sm.add_constant(X)  # Add intercept\ny = data_complete['y']\n\n# Fit OLS model\nmodel = sm.OLS(y, X).fit()\n\n# Display results\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.631\nModel:                            OLS   Adj. R-squared:                  0.629\nMethod:                 Least Squares   F-statistic:                     295.0\nDate:                Sun, 06 Apr 2025   Prob (F-statistic):          1.19e-111\nTime:                        13:35:55   Log-Likelihood:                -4541.7\nNo. Observations:                 522   AIC:                             9091.\nDf Residuals:                     518   BIC:                             9108.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4652.1226   2050.412      2.269      0.024     623.978    8680.268\nx1            20.5814      5.133      4.010      0.000      10.498      30.665\nx2            39.7328      1.342     29.605      0.000      37.096      42.369\nx3            55.6552     13.271      4.194      0.000      29.584      81.726\n==============================================================================\nOmnibus:                        4.773   Durbin-Watson:                   1.958\nProb(Omnibus):                  0.092   Jarque-Bera (JB):                4.593\nSkew:                           0.223   Prob(JB):                        0.101\nKurtosis:                       3.108   Cond. No.                     8.24e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 8.24e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nNow we move on to a MAR model. First starting with Stata.\n\n%%stata\n\nclear\n\n* Set seed\nset seed 123\n\n* Set number of observations\nset obs 1000\n\n* Generate metric independent variable #1\ndrawnorm x1, n(1000) means(40) sds(12)\n\n* Generate metric independent variable #2\ndrawnorm x2, n(1000) means(200) sds(50)\n\n* Generate metric independent variable #3\ndrawnorm x3, n(1000) means(150) sds(5)\n\n* Generate metric dependent variable with large coefficients\ngen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n* Generate MAR\ngen prob_mar = logistic(y-21791)\ngen rmar = 0 if prob_mar==0\nreplace x2 = . if rmar == 0  // Set x to missing where rmar == 0\nregress y x1 x2 x3\n\netable, append\n\ntwoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\") legend(off)\n\n\n. \n. clear\n\n. \n. * Set seed\n. set seed 123\n\n. \n. * Set number of observations\n. set obs 1000\nNumber of observations (_N) was 0, now 1,000.\n\n. \n. * Generate metric independent variable #1\n. drawnorm x1, n(1000) means(40) sds(12)\n\n. \n. * Generate metric independent variable #2\n. drawnorm x2, n(1000) means(200) sds(50)\n\n. \n. * Generate metric independent variable #3\n. drawnorm x3, n(1000) means(150) sds(5)\n\n. \n. * Generate metric dependent variable with large coefficients\n. gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n. \n. * Generate MAR\n. gen prob_mar = logistic(y-21791)\n\n. gen rmar = 0 if prob_mar==0\n(500 missing values generated)\n\n. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n(500 real changes made, 500 to missing)\n\n. regress y x1 x2 x3\n\n      Source |       SS           df       MS      Number of obs   =       500\n-------------+----------------------------------   F(3, 496)       =    122.89\n       Model |   552925039         3   184308346   Prob &gt; F        =    0.0000\n    Residual |   743864709       496  1499727.24   R-squared       =    0.4264\n-------------+----------------------------------   Adj R-squared   =    0.4229\n       Total |  1.2968e+09       499  2598777.05   Root MSE        =    1224.6\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   20.14886   4.982177     4.04   0.000     10.36009    29.93763\n          x2 |   26.96303   1.407434    19.16   0.000     24.19777     29.7283\n          x3 |    19.7006   10.99788     1.79   0.074    -1.907582    41.30878\n       _cons |   13742.28   1717.077     8.00   0.000     10368.64    17115.92\n------------------------------------------------------------------------------\n\n. \n. etable, append\n\n-------------------------------------------------------------------\n                            y          2           y          y    \n-------------------------------------------------------------------\nx1                         29.409      29.409     25.214     20.149\n                          (4.032)     (4.024)    (5.942)    (4.982)\nx2                         42.717      42.717     42.557     26.963\n                          (0.944)     (0.942)    (1.385)    (1.407)\nx3                         54.520      54.520     48.179     19.701\n                          (9.246)     (9.227)   (13.133)   (10.998)\nIntercept                3778.896    3778.896   4912.711  13742.276\n                       (1402.667)  (1399.858) (2007.467) (1717.077)\nvar(e.y)                             2.20e+06                      \n                                  (98396.280)                      \nNumber of observations       1000        1000        508        500\n-------------------------------------------------------------------\n\n. \n. twoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\"\n&gt; ) legend(off)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%R\n\n# Load necessary library\nset.seed(123)  # Ensure reproducibility\n\n# Generate metric independent variables\nx1 &lt;- rnorm(1000, mean = 40, sd = 12)\nx2 &lt;- rnorm(1000, mean = 200, sd = 50)\nx3 &lt;- rnorm(1000, mean = 150, sd = 5)\n\n# Generate metric dependent variable\ny &lt;- 30*x1 + 40*x2 + 50*x3 + rnorm(1000, mean = 5000, sd = 1500)\n\n# Generate MAR (Missing At Random) pattern\nprob_mar &lt;- 1 / (1 + exp(-(y - 21791)))  # Logistic function\nrmar &lt;- rbinom(1000, 1, prob_mar)  # Generate missing indicator based on probability\nx2[rmar == 0] &lt;- NA  # Set x2 to NA for MAR missingness\n\n# Create data frame\ndata &lt;- data.frame(y, x1, x2, x3)\n\n# Run OLS regression (listwise deletion by default)\nols_model &lt;- lm(y ~ x1 + x2 + x3, data = data, na.action = na.omit)\n\n# Display regression results\nsummary(ols_model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data, na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2876.0  -828.4   -78.3   670.2  3761.0 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12209.232   1598.736   7.637 1.13e-13 ***\nx1             19.693      4.588   4.292 2.12e-05 ***\nx2             24.731      1.351  18.300  &lt; 2e-16 ***\nx3             33.021     10.191   3.240  0.00127 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1161 on 504 degrees of freedom\n  (492 observations deleted due to missingness)\nMultiple R-squared:  0.4173,    Adjusted R-squared:  0.4138 \nF-statistic: 120.3 on 3 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Set number of observations\nn = 1000\n\n# Generate independent variables\nx1 = np.random.normal(40, 12, n)\nx2 = np.random.normal(200, 50, n)\nx3 = np.random.normal(150, 5, n)\n\n# Generate dependent variable\ny = 3*x1 + 4*x2 + 4*x3 + np.random.normal(500, 150, n)\n\n# Generate MAR (Missing At Random) for x2\n\n# Logistic function with scaled y values\nprob_mar = 1 / (1 + np.exp(y-2050))  # Apply logistic function\nrmar = np.random.binomial(1, prob_mar)  # Generate MAR indicator\nx2[rmar == 0] = np.nan  # Set x2 to missing\n\n# Create DataFrame\ndata = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n\n# Run OLS regression (listwise deletion by default)\ndata_clean = data.dropna()  # Remove rows with missing values\nX = sm.add_constant(data_clean[['x1', 'x2', 'x3']])  # Add intercept\nmodel = sm.OLS(data_clean['y'], X).fit()\n\n# Display regression results\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.398\nModel:                            OLS   Adj. R-squared:                  0.395\nMethod:                 Least Squares   F-statistic:                     116.1\nDate:                Sun, 06 Apr 2025   Prob (F-statistic):           9.95e-58\nTime:                        13:35:55   Log-Likelihood:                -3262.9\nNo. Observations:                 531   AIC:                             6534.\nDf Residuals:                     527   BIC:                             6551.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        872.2379    153.985      5.664      0.000     569.737    1174.738\nx1             1.9156      0.407      4.708      0.000       1.116       2.715\nx2             2.3273      0.128     18.251      0.000       2.077       2.578\nx3             3.3532      1.005      3.338      0.001       1.380       5.327\n==============================================================================\nOmnibus:                        7.685   Durbin-Watson:                   1.802\nProb(Omnibus):                  0.021   Jarque-Bera (JB):                7.560\nSkew:                          -0.275   Prob(JB):                       0.0228\nKurtosis:                       3.201   Cond. No.                     7.31e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 7.31e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nNow we have our baseline MAR model for each software, now we can focus on implementing potential handling missing data solutions. Starting with Stata and Single mean imputation.\n\n%%stata\n\nclear\n\n* Set seed\nset seed 123\n\n* Set number of observations\nset obs 1000\n\n* Generate metric independent variable #1\ndrawnorm x1, n(1000) means(40) sds(12)\n\n* Generate metric independent variable #2\ndrawnorm x2, n(1000) means(200) sds(50)\n\n* Generate metric independent variable #3\ndrawnorm x3, n(1000) means(150) sds(5)\n\n* Generate metric dependent variable with large coefficients\ngen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n* Generate MAR\ngen prob_mar = logistic(y-21791)\ngen rmar = 0 if prob_mar==0\nreplace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n\nsummarize x2, meanonly\nreplace x2 = r(mean) if missing(x2)\n\nregress y x1 x2 x3\n\netable, append\n\ntwoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\") legend(off)\n\n\n. \n. clear\n\n. \n. * Set seed\n. set seed 123\n\n. \n. * Set number of observations\n. set obs 1000\nNumber of observations (_N) was 0, now 1,000.\n\n. \n. * Generate metric independent variable #1\n. drawnorm x1, n(1000) means(40) sds(12)\n\n. \n. * Generate metric independent variable #2\n. drawnorm x2, n(1000) means(200) sds(50)\n\n. \n. * Generate metric independent variable #3\n. drawnorm x3, n(1000) means(150) sds(5)\n\n. \n. * Generate metric dependent variable with large coefficients\n. gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n. \n. * Generate MAR\n. gen prob_mar = logistic(y-21791)\n\n. gen rmar = 0 if prob_mar==0\n(500 missing values generated)\n\n. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n(500 real changes made, 500 to missing)\n\n. \n. summarize x2, meanonly\n\n. replace x2 = r(mean) if missing(x2)\n(500 real changes made)\n\n. \n. regress y x1 x2 x3\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(3, 996)       =     44.23\n       Model |   814884957         3   271628319   Prob &gt; F        =    0.0000\n    Residual |  6.1174e+09       996  6141946.06   R-squared       =    0.1175\n-------------+----------------------------------   Adj R-squared   =    0.1149\n       Total |  6.9323e+09       999  6939202.43   Root MSE        =    2478.3\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   36.86274   6.772257     5.44   0.000     23.57321    50.15227\n          x2 |   28.00538    2.82263     9.92   0.000      22.4664    33.54436\n          x3 |    63.7622   15.41944     4.14   0.000     33.50389    94.02052\n       _cons |   4173.834   2437.463     1.71   0.087    -609.3184    8956.987\n------------------------------------------------------------------------------\n\n. \n. etable, append\n\n------------------------------------------------------------------------------\n                            y          2           y          y          y    \n------------------------------------------------------------------------------\nx1                         29.409      29.409     25.214     20.149     36.863\n                          (4.032)     (4.024)    (5.942)    (4.982)    (6.772)\nx2                         42.717      42.717     42.557     26.963     28.005\n                          (0.944)     (0.942)    (1.385)    (1.407)    (2.823)\nx3                         54.520      54.520     48.179     19.701     63.762\n                          (9.246)     (9.227)   (13.133)   (10.998)   (15.419)\nIntercept                3778.896    3778.896   4912.711  13742.276   4173.834\n                       (1402.667)  (1399.858) (2007.467) (1717.077) (2437.463)\nvar(e.y)                             2.20e+06                                 \n                                  (98396.280)                                 \nNumber of observations       1000        1000        508        500       1000\n------------------------------------------------------------------------------\n\n. \n. twoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\"\n&gt; ) legend(off)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%R\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate metric independent variables\nx1 &lt;- rnorm(1000, mean = 40, sd = 12)\nx2 &lt;- rnorm(1000, mean = 200, sd = 50)\nx3 &lt;- rnorm(1000, mean = 150, sd = 5)\n\n# Generate metric dependent variable\ny &lt;- 30*x1 + 40*x2 + 50*x3 + rnorm(1000, mean = 5000, sd = 1500)\n\n# Generate MAR (Missing At Random) for x2 using logistic function\nprob_mar &lt;- 1 / (1 + exp(-(y - 21791)))  # Logistic function\nrmar &lt;- rbinom(1000, 1, prob_mar)  # Generate MAR indicator\nx2[rmar == 0] &lt;- NA  # Set x2 to missing where rmar == 0\n\n# Display the summary statistics for x2 (mean only)\nmean_x2 &lt;- mean(x2, na.rm = TRUE)\n\n# Replace missing values of x2 with the mean of x2\nx2[is.na(x2)] &lt;- mean_x2\n\n# Create a data frame with the variables\ndata &lt;- data.frame(y, x1, x2, x3)\n\n# Run OLS regression\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = data)\n\n# Display the regression results\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7956.9 -1644.3   163.7  1915.5  5959.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1910.091   2459.762   0.777    0.438    \nx1            43.837      6.384   6.866 1.16e-11 ***\nx2            24.910      2.794   8.915  &lt; 2e-16 ***\nx3            81.888     15.540   5.270 1.68e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2401 on 996 degrees of freedom\nMultiple R-squared:  0.1319,    Adjusted R-squared:  0.1293 \nF-statistic: 50.46 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Set number of observations\nn = 1000\n\n# Generate independent variables\nx1 = np.random.normal(40, 12, n)\nx2 = np.random.normal(200, 50, n)\nx3 = np.random.normal(150, 5, n)\n\n# Generate dependent variable\ny = 3*x1 + 4*x2 + 4*x3 + np.random.normal(500, 150, n)\n\n# Generate MAR (Missing At Random) for x2\n\n# Logistic function with scaled y values\nprob_mar = 1 / (1 + np.exp(y-2050))  # Apply logistic function\nrmar = np.random.binomial(1, prob_mar)  # Generate MAR indicator\nx2[rmar == 0] = np.nan  # Set x2 to missing\n\n# Replace missing values in x2 with the mean of x2\nx2[np.isnan(x2)] = np.nanmean(x2)\n\n# Create DataFrame\ndata = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n\n# Run OLS regression (listwise deletion by default)\nX = sm.add_constant(data[['x1', 'x2', 'x3']])  # Add intercept\nmodel = sm.OLS(data['y'], X).fit()\n\n# Display regression results\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.094\nModel:                            OLS   Adj. R-squared:                  0.091\nMethod:                 Least Squares   F-statistic:                     34.32\nDate:                Sun, 06 Apr 2025   Prob (F-statistic):           4.11e-21\nTime:                        13:35:55   Log-Likelihood:                -6848.0\nNo. Observations:                1000   AIC:                         1.370e+04\nDf Residuals:                     996   BIC:                         1.372e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        958.4029    228.310      4.198      0.000     510.379    1406.427\nx1             2.6749      0.604      4.426      0.000       1.489       3.861\nx2             2.3577      0.256      9.202      0.000       1.855       2.860\nx3             3.7336      1.473      2.535      0.011       0.844       6.623\n==============================================================================\nOmnibus:                       45.270   Durbin-Watson:                   1.968\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               29.371\nSkew:                           0.292   Prob(JB):                     4.19e-07\nKurtosis:                       2.398   Cond. No.                     7.36e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 7.36e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nNow we move on to FIML approaches with Stata first.\n\n%%stata\n\nclear\n\n* Set seed\nset seed 123\n\n* Set number of observations\nset obs 1000\n\n* Generate metric independent variable #1\ndrawnorm x1, n(1000) means(40) sds(12)\n\n* Generate metric independent variable #2\ndrawnorm x2, n(1000) means(200) sds(50)\n\n* Generate metric independent variable #3\ndrawnorm x3, n(1000) means(150) sds(5)\n\n* Generate metric dependent variable with large coefficients\ngen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n* Generate MAR\ngen prob_mar = logistic(y-21791)\ngen rmar = 0 if prob_mar==0\nreplace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n    \n* Model estimation\nsem (y &lt;- x1 x2 x3), method(mlmv)\n\netable, append\n\ntwoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\") legend(off)\n\n\n. \n. clear\n\n. \n. * Set seed\n. set seed 123\n\n. \n. * Set number of observations\n. set obs 1000\nNumber of observations (_N) was 0, now 1,000.\n\n. \n. * Generate metric independent variable #1\n. drawnorm x1, n(1000) means(40) sds(12)\n\n. \n. * Generate metric independent variable #2\n. drawnorm x2, n(1000) means(200) sds(50)\n\n. \n. * Generate metric independent variable #3\n. drawnorm x3, n(1000) means(150) sds(5)\n\n. \n. * Generate metric dependent variable with large coefficients\n. gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n. \n. * Generate MAR\n. gen prob_mar = logistic(y-21791)\n\n. gen rmar = 0 if prob_mar==0\n(500 missing values generated)\n\n. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n(500 real changes made, 500 to missing)\n\n.     \n. * Model estimation\n. sem (y &lt;- x1 x2 x3), method(mlmv)\nnote: Missing values found in observed exogenous variables. Using the\n      noxconditional behavior. Specify the forcexconditional option to\n      override this behavior.\nEndogenous variables\n  Observed: y\n\nExogenous variables\n  Observed: x1 x2 x3\n\nFitting saturated model:\nIteration 0:  Log likelihood = -19809.893  \nIteration 1:  Log likelihood = -18956.184  \nIteration 2:  Log likelihood = -18603.225  \nIteration 3:  Log likelihood = -18601.025  \nIteration 4:  Log likelihood =     -18601  \nIteration 5:  Log likelihood =     -18601  \n\nFitting baseline model:\nIteration 0:  Log likelihood = -18886.253  \nIteration 1:  Log likelihood =  -18792.75  \nIteration 2:  Log likelihood =  -18757.19  \nIteration 3:  Log likelihood = -18754.887  \nIteration 4:  Log likelihood = -18754.864  \nIteration 5:  Log likelihood = -18754.864  \n\nFitting target model:\nIteration 0:  Log likelihood =     -18601  \nIteration 1:  Log likelihood =     -18601  \n\nStructural equation model                                Number of obs = 1,000\nEstimation method: mlmv\n\nLog likelihood = -18601\n\n------------------------------------------------------------------------------\n             |                 OIM\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nStructural   |\n  y          |\n          x1 |   38.10574   5.583454     6.82   0.000     27.16237    49.04911\n          x2 |   41.70024   1.300009    32.08   0.000     39.15227    44.24821\n          x3 |   40.31928   12.42463     3.25   0.001     15.96746     64.6711\n       _cons |   5775.744   1925.672     3.00   0.003     2001.496    9549.992\n-------------+----------------------------------------------------------------\n     mean(x1)|   40.24597   .3686473   109.17   0.000     39.52343    40.96851\n     mean(x2)|   200.6471   2.530853    79.28   0.000     195.6867    205.6075\n     mean(x3)|    149.797   .1607991   931.58   0.000     149.4818    150.1121\n-------------+----------------------------------------------------------------\n     var(e.y)|    2300879   187511.7                       1961212     2699374\n      var(x1)|   135.9009   6.077671                       124.496    148.3505\n      var(x2)|    2554.76   197.2925                      2195.916    2972.244\n      var(x3)|   25.85634   1.156331                      23.68646      28.225\n-------------+----------------------------------------------------------------\n   cov(x1,x2)|  -29.66936    22.2683    -1.33   0.183    -73.31444    13.97571\n   cov(x1,x3)|   1.100681   1.874863     0.59   0.557    -2.573983    4.775344\n   cov(x2,x3)|   12.02246   9.573148     1.26   0.209    -6.740568    30.78548\n------------------------------------------------------------------------------\nLR test of model vs. saturated: chi2(0) = 0.00                 Prob &gt; chi2 = .\n\n. \n. etable, append\n\n-----------------------------------------------------------------------------------------\n                            y          2           y          y          y          6    \n-----------------------------------------------------------------------------------------\nx1                         29.409      29.409     25.214     20.149     36.863     38.106\n                          (4.032)     (4.024)    (5.942)    (4.982)    (6.772)    (5.583)\nx2                         42.717      42.717     42.557     26.963     28.005     41.700\n                          (0.944)     (0.942)    (1.385)    (1.407)    (2.823)    (1.300)\nx3                         54.520      54.520     48.179     19.701     63.762     40.319\n                          (9.246)     (9.227)   (13.133)   (10.998)   (15.419)   (12.425)\nIntercept                3778.896    3778.896   4912.711  13742.276   4173.834   5775.744\n                       (1402.667)  (1399.858) (2007.467) (1717.077) (2437.463) (1925.672)\nvar(e.y)                             2.20e+06                                    2.30e+06\n                                  (98396.280)                                  (1.88e+05)\nmean(x1)                                                                           40.246\n                                                                                  (0.369)\nmean(x2)                                                                          200.647\n                                                                                  (2.531)\nmean(x3)                                                                          149.797\n                                                                                  (0.161)\nvar(x1)                                                                           135.901\n                                                                                  (6.078)\nvar(x2)                                                                          2554.760\n                                                                                (197.292)\nvar(x3)                                                                            25.856\n                                                                                  (1.156)\ncov(x1,x2)                                                                        -29.669\n                                                                                 (22.268)\ncov(x1,x3)                                                                          1.101\n                                                                                  (1.875)\ncov(x2,x3)                                                                         12.022\n                                                                                  (9.573)\nNumber of observations       1000        1000        508        500       1000       1000\n-----------------------------------------------------------------------------------------\n\n. \n. twoway (scatter y x2) (lfit y x2), title(\"Scatter Plot with Line of Best Fit\"\n&gt; ) legend(off)\n\n. \n\n\n\n\n\n\n\n\n\n\n%%R\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate metric independent variables\nx1 &lt;- rnorm(1000, mean = 40, sd = 12)\nx2 &lt;- rnorm(1000, mean = 200, sd = 50)\nx3 &lt;- rnorm(1000, mean = 150, sd = 5)\n\n# Generate metric dependent variable\ny &lt;- 30*x1 + 40*x2 + 50*x3 + rnorm(1000, mean = 5000, sd = 1500)\n\n# Generate MAR (Missing At Random) for x2\nprob_mar &lt;- 1 / (1 + exp(-(y - 21791)))  # Logistic function\nrmar &lt;- ifelse(prob_mar == 0, 0, 1)  # 0 if prob_mar is 0, 1 otherwise\nx2[rmar == 0] &lt;- NA  # Set x2 to missing where rmar == 0\n\n# Load 'lavaan' package for SEM (Structural Equation Modeling)\nlibrary(lavaan)\n\n# Define the model formula (SEM model)\nmodel &lt;- '\n  y ~ x1 + x2 + x3  # y as a function of x1, x2, and x3\n'\n\n# Fit the model using the 'lavaan' package\nfit &lt;- sem(model, data = data.frame(y, x1, x2, x3))\n\n# Display the model summary\nsummary(fit)\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         4\n\n                                                  Used       Total\n  Number of observations                           606        1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate     Std.Err   z-value  P(&gt;|z|)\n  y ~                                                     \n    x1                  19.799     4.270    4.637    0.000\n    x2                  27.159     1.260   21.549    0.000\n    x3                  41.840     9.653    4.334    0.000\n\nVariances:\n                   Estimate     Std.Err   z-value  P(&gt;|z|)\n   .y              1469996.086 84449.069   17.407    0.000\n\n\n\nIn addition: Warning message:\nlavaan-&gt;lav_data_full():  \n   some observed variances are larger than 1000000 use varTable(fit) to \n   investigate \n\n\nTo my knowledge, there is no viable FIML estimation practice within Python. This makes it impossible to use FIML as a handling missing data technique in Python and as such you have to use alternative methods such as Imputation. This will now be demonstrated, again starting with Stata. For the purposes of this simulation, as all variables are continuous a MVN Multiple Imputation approach can be used, though MICE allows for variables of different types to be imputed.\n\n%%stata\n\nclear\n\n* Set seed\nset seed 123\n\n* Set number of observations\nset obs 1000\n\n* Generate metric independent variable #1\ndrawnorm x1, n(1000) means(40) sds(12)\n\n* Generate metric independent variable #2\ndrawnorm x2, n(1000) means(200) sds(50)\n\n* Generate metric independent variable #3\ndrawnorm x3, n(1000) means(150) sds(5)\n\n* Generate metric dependent variable with large coefficients\ngen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n* Generate MAR\ngen prob_mar = logistic(y-21791)\ngen rmar = 0 if prob_mar==0\nreplace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n    \nmi set wide\n\nmi register imputed y x1 x2 x3\n\ntab _mi_miss\n\n\nmi impute chained ///\n///\n(regress) y x1 x2 x3 ///\n, rseed(12345) dots force add(10) burnin(10) \n\n\nmi estimate, post dots: regress y x1 x2 x3\n\netable, append\n\n\n. \n. clear\n\n. \n. * Set seed\n. set seed 123\n\n. \n. * Set number of observations\n. set obs 1000\nNumber of observations (_N) was 0, now 1,000.\n\n. \n. * Generate metric independent variable #1\n. drawnorm x1, n(1000) means(40) sds(12)\n\n. \n. * Generate metric independent variable #2\n. drawnorm x2, n(1000) means(200) sds(50)\n\n. \n. * Generate metric independent variable #3\n. drawnorm x3, n(1000) means(150) sds(5)\n\n. \n. * Generate metric dependent variable with large coefficients\n. gen y = 30*x1 + 40*x2 + 50*x3 + rnormal(5000, 1500)\n\n. \n. * Generate MAR\n. gen prob_mar = logistic(y-21791)\n\n. gen rmar = 0 if prob_mar==0\n(500 missing values generated)\n\n. replace x2 = . if rmar == 0  // Set x to missing where rmar == 0\n(500 real changes made, 500 to missing)\n\n.     \n. mi set wide\n\n. \n. mi register imputed y x1 x2 x3\n\n. \n. tab _mi_miss\n\n Incomplete |\nobservation |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          0 |        500       50.00       50.00\n          1 |        500       50.00      100.00\n------------+-----------------------------------\n      Total |      1,000      100.00\n\n. \n. \n. mi impute chained ///\n&gt; ///\n&gt; (regress) y x1 x2 x3 ///\n&gt; , rseed(12345) dots force add(10) burnin(10) \nnote: variables y x1 x3 contain no soft missing (.) values; imputing nothing\nnote: missing-value pattern is monotone; no iteration performed.\n\nConditional models (monotone):\n                x2: regress x2 x3 x1 y\n\nPerforming chained iterations:\n  imputing m=1 through m=10 .........10 done\n\nMultivariate imputation                     Imputations =       10\nChained equations                                 added =       10\nImputed: m=1 through m=10                       updated =        0\n\nInitialization: monotone                     Iterations =        0\n                                                burn-in =        0\n\n                 y: linear regression\n                x1: linear regression\n                x2: linear regression\n                x3: linear regression\n\n------------------------------------------------------------------\n                   |               Observations per m             \n                   |----------------------------------------------\n          Variable |   Complete   Incomplete   Imputed |     Total\n-------------------+-----------------------------------+----------\n                 y |       1000            0         0 |      1000\n                x1 |       1000            0         0 |      1000\n                x2 |        500          500       500 |      1000\n                x3 |       1000            0         0 |      1000\n------------------------------------------------------------------\n(Complete + Incomplete = Total; Imputed is the minimum across m\n of the number of filled-in observations.)\n\n. \n. \n. mi estimate, post dots: regress y x1 x2 x3\n\nImputations (10):\n  .........10 done\n\nMultiple-imputation estimates                   Imputations       =         10\nLinear regression                               Number of obs     =      1,000\n                                                Average RVI       =     2.3440\n                                                Largest FMI       =     0.5779\n                                                Complete DF       =        996\nDF adjustment:   Small sample                   DF:     min       =      27.94\n                                                        avg       =      60.86\n                                                        max       =     124.57\nModel F test:       Equal FMI                   F(   3,  106.8)   =     365.04\nWithin VCE type:          OLS                   Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   36.94599   4.821662     7.66   0.000       27.403    46.48898\n          x2 |    42.0321   1.471605    28.56   0.000     39.01734    45.04686\n          x3 |   39.50875   12.21717     3.23   0.002     15.03525    63.98224\n       _cons |   5850.553    2031.63     2.88   0.007     1725.712    9975.395\n------------------------------------------------------------------------------\n\n. \n. etable, append\n\n----------------------------------------------------------------------------------------------------\n                            y          2           y          y          y          6          y    \n----------------------------------------------------------------------------------------------------\nx1                         29.409      29.409     25.214     20.149     36.863     38.106     36.946\n                          (4.032)     (4.024)    (5.942)    (4.982)    (6.772)    (5.583)    (4.822)\nx2                         42.717      42.717     42.557     26.963     28.005     41.700     42.032\n                          (0.944)     (0.942)    (1.385)    (1.407)    (2.823)    (1.300)    (1.472)\nx3                         54.520      54.520     48.179     19.701     63.762     40.319     39.509\n                          (9.246)     (9.227)   (13.133)   (10.998)   (15.419)   (12.425)   (12.217)\nIntercept                3778.896    3778.896   4912.711  13742.276   4173.834   5775.744   5850.553\n                       (1402.667)  (1399.858) (2007.467) (1717.077) (2437.463) (1925.672) (2031.630)\nvar(e.y)                             2.20e+06                                    2.30e+06           \n                                  (98396.280)                                  (1.88e+05)           \nmean(x1)                                                                           40.246           \n                                                                                  (0.369)           \nmean(x2)                                                                          200.647           \n                                                                                  (2.531)           \nmean(x3)                                                                          149.797           \n                                                                                  (0.161)           \nvar(x1)                                                                           135.901           \n                                                                                  (6.078)           \nvar(x2)                                                                          2554.760           \n                                                                                (197.292)           \nvar(x3)                                                                            25.856           \n                                                                                  (1.156)           \ncov(x1,x2)                                                                        -29.669           \n                                                                                 (22.268)           \ncov(x1,x3)                                                                          1.101           \n                                                                                  (1.875)           \ncov(x2,x3)                                                                         12.022           \n                                                                                  (9.573)           \nNumber of observations       1000        1000        508        500       1000       1000       1000\n----------------------------------------------------------------------------------------------------\n\n. \n\n\nNow R code.\n\n%%R\n# Install and load the `mice` package for multiple imputation (if not already installed)\nif (!require(mice)) install.packages(\"mice\", dependencies = TRUE)\nlibrary(mice)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate metric independent variables\nx1 &lt;- rnorm(1000, mean = 40, sd = 12)\nx2 &lt;- rnorm(1000, mean = 200, sd = 50)\nx3 &lt;- rnorm(1000, mean = 150, sd = 5)\n\n# Generate metric dependent variable\ny &lt;- 30*x1 + 40*x2 + 50*x3 + rnorm(1000, mean = 5000, sd = 1500)\n\n# Generate MAR (Missing At Random) for x2\nprob_mar &lt;- 1 / (1 + exp(-(y - 21791)))  # Logistic function\nrmar &lt;- ifelse(prob_mar == 0, 0, 1)  # 0 if prob_mar is 0, 1 otherwise\nx2[rmar == 0] &lt;- NA  # Set x2 to missing where rmar == 0\n\n# Prepare the data for multiple imputation\ndata &lt;- data.frame(y, x1, x2, x3)\n\n# Perform multiple imputation using chained equations (MICE)\nimp &lt;- mice(data, m = 10, method = \"pmm\", seed = 12345)\n\n# Perform regression after imputation\nfit &lt;- with(imp, lm(y ~ x1 + x2 + x3))\n\n# Combine the results of the regression and display the estimates\npooled_results &lt;- pool(fit)\nsummary(pooled_results)\n\n\n iter imp variable\n  1   1  x2\n  1   2  x2\n  1   3  x2\n  1   4  x2\n  1   5  x2\n  1   6  x2\n  1   7  x2\n  1   8  x2\n  1   9  x2\n  1   10  x2\n  2   1  x2\n  2   2  x2\n  2   3  x2\n  2   4  x2\n  2   5  x2\n  2   6  x2\n  2   7  x2\n  2   8  x2\n  2   9  x2\n  2   10  x2\n  3   1  x2\n  3   2  x2\n  3   3  x2\n  3   4  x2\n  3   5  x2\n  3   6  x2\n  3   7  x2\n  3   8  x2\n  3   9  x2\n  3   10  x2\n  4   1  x2\n  4   2  x2\n  4   3  x2\n  4   4  x2\n  4   5  x2\n  4   6  x2\n  4   7  x2\n  4   8  x2\n  4   9  x2\n  4   10  x2\n  5   1  x2\n  5   2  x2\n  5   3  x2\n  5   4  x2\n  5   5  x2\n  5   6  x2\n  5   7  x2\n  5   8  x2\n  5   9  x2\n  5   10  x2\n         term   estimate   std.error statistic       df      p.value\n1 (Intercept) 2184.92911 1901.113549  1.149289 415.6189 2.510977e-01\n2          x1   30.19575    5.472728  5.517495 165.8509 1.299693e-07\n3          x2   39.85730    1.632656 24.412548  80.6700 5.233432e-39\n4          x3   67.14173   12.700619  5.286493 308.6763 2.363451e-07\n\n\nLoading required package: mice\n\nAttaching package: ‘mice’\n\nThe following object is masked from ‘package:stats’:\n\n    filter\n\nThe following objects are masked from ‘package:base’:\n\n    cbind, rbind\n\n\n\nAnd finally, python. (not currently working)"
  },
  {
    "objectID": "teaching/index.html#multi-level-modelling-in-social-science-ssps10024-pgsp11424",
    "href": "teaching/index.html#multi-level-modelling-in-social-science-ssps10024-pgsp11424",
    "title": "Teaching",
    "section": "",
    "text": "Description:\nThe course enables students to understand and use multilevel models mainly in the context of social science, but examples are also given from medicine and some aspects of biological science.\nDate: 2025, Semester 2\nCourse Catalogue"
  }
]